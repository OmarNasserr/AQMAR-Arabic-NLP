{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL5zl_b39Vhw"
      },
      "source": [
        "# **Downloading required data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLVV0YVz9LsD",
        "outputId": "e2e49a41-9ce7-4cbb-b7a9-bedd847f249f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot open cookies file ‘/tmp/cookies.txt’: No such file or directory\n",
            "--2023-04-07 02:43:59--  http://www.cs.cmu.edu/~ark/ArabicNER/AQMAR_Arabic_NER_corpus-1.0.zip\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7815886 (7.5M) [application/zip]\n",
            "Saving to: ‘AQMAR_Arabic_NER_corpus-1.0.zip’\n",
            "\n",
            "AQMAR_Arabic_NER_co 100%[===================>]   7.45M  1.12MB/s    in 7.6s    \n",
            "\n",
            "2023-04-07 02:44:07 (999 KB/s) - ‘AQMAR_Arabic_NER_corpus-1.0.zip’ saved [7815886/7815886]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"http://www.cs.cmu.edu/~ark/ArabicNER/AQMAR_Arabic_NER_corpus-1.0.zip\" -O AQMAR_Arabic_NER_corpus-1.0.zip && rm -rf /tmp/cookies.txt\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"AQMAR_Arabic_NER_corpus-1.0.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"AQMAR_UnZip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qUkKBiD9m1W"
      },
      "source": [
        "# **Downloading Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8rijYll9fMv",
        "outputId": "ce2c5c80-57aa-442d-a762-78f7d3633a30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot open cookies file ‘/tmp/cookies.txt’: No such file or directory\n",
            "--2023-04-07 02:44:07--  https://bakrianoo.ewr1.vultrobjects.com/aravec/full_uni_cbow_300_twitter.zip\n",
            "Resolving bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)... 108.61.0.122, 2001:19f0:0:22::100\n",
            "Connecting to bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)|108.61.0.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2833686412 (2.6G) [application/zip]\n",
            "Saving to: ‘full_uni_cbow_300_twitter.zip’\n",
            "\n",
            "full_uni_cbow_300_t 100%[===================>]   2.64G  44.6MB/s    in 66s     \n",
            "\n",
            "2023-04-07 02:45:14 (40.7 MB/s) - ‘full_uni_cbow_300_twitter.zip’ saved [2833686412/2833686412]\n",
            "\n",
            "Archive:  full_uni_cbow_300_twitter.zip\n",
            "  inflating: full_uni_cbow_300_twitter.mdl  \n",
            "  inflating: full_uni_cbow_300_twitter.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_uni_cbow_300_twitter.mdl.wv.vectors.npy  \n"
          ]
        }
      ],
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://bakrianoo.ewr1.vultrobjects.com/aravec/full_uni_cbow_300_twitter.zip\" -O full_uni_cbow_300_twitter.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip full_uni_cbow_300_twitter.zip "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXZo3Lo_92Ft"
      },
      "source": [
        "# **Split file to text and label**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A2MPDTac90u4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def split_text_label(filename):\n",
        "    f = open(filename, encoding=\"utf-8\")\n",
        "    split_labeled_text = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        # If the line is empty or starts with \"-DOCSTART\" or is a newline character, \n",
        "        # then add the current sentence to split_labeled_text list if it is not empty, \n",
        "        # then reset the sentence list and continue to the next line.\n",
        "        if len(line) == 0 or line.startswith(\"-DOCSTART\") or line[0] == \"\\n\":\n",
        "            if len(sentence) > 0:\n",
        "                split_labeled_text.append(sentence)\n",
        "                sentence = []\n",
        "            continue\n",
        "        # Split the current line by tab character and store it in splits list\n",
        "        splits = line.split(\"\\t\")\n",
        "        # Remove the newline character from the last element of the splits list and store it in label variable\n",
        "        label = splits[-1].rstrip(\"\\n\")\n",
        "\n",
        "        #\"O\" indicates that there is no entity present in that word/token.\n",
        "        #\"B-MIS1\" and \"B-MIS2\" indicate the beginning of an entity of type \"MIS1\" or \"MIS2\", respectively.\n",
        "        #\"I-ORG\" indicates a continuation of an entity of type \"ORG\".\n",
        "        if label == \"IO\" or label == \"OO\":\n",
        "            label = \"O\"\n",
        "        elif label == \"B-MIS-1\" or label == \"B-MIS1`\" or label == \"B-MISS1\":\n",
        "            label = \"B-MIS1\"\n",
        "        elif label == \"B-MIS-2\":\n",
        "            label = \"B-MIS2\"\n",
        "        elif label == \"I--ORG\":\n",
        "            label = \"I-ORG\"\n",
        "        elif label == \"\":\n",
        "            continue\n",
        "        sentence.append([splits[0], label])\n",
        "    # If the last sentence is not empty, then add it to split_labeled_text list\n",
        "    if len(sentence) > 0:\n",
        "        split_labeled_text.append(sentence)\n",
        "        sentence = []\n",
        "    return split_labeled_text\n",
        "\n",
        "\n",
        "split_train = split_text_label(\"./AQMAR_UnZip/featureFiles/test/all.test.features.txt\")\n",
        "split_test = split_text_label(\"./AQMAR_UnZip/featureFiles/dev/all.dev.features.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FNdZTN8U-0wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8729973-08e0-4b3d-baa3-3b11ebcfeee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1976,)\n",
            "(711,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-9d71bc702c2d>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  print(np.array(split_train).shape)\n",
            "<ipython-input-5-9d71bc702c2d>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  print(np.array(split_test).shape)\n"
          ]
        }
      ],
      "source": [
        "print(np.array(split_train).shape)\n",
        "print(np.array(split_test).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAbDqg3V_AXD"
      },
      "source": [
        "# **Preprocessing data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NDwWH4kk_Br1"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def normalize_arabic(text):\n",
        "    # This function replaces Arabic letters that have different forms in Unicode\n",
        "    # with the standard form.\n",
        "    # For example, it replaces Alef variants and Hamza with the standard Alef letter.\n",
        "\n",
        "\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocessing(data_list, model):\n",
        "    # This function preprocesses the input data to prepare it for training or testing.\n",
        "    # It performs the following steps:\n",
        "    # - Remove non-Arabic words from each sentence\n",
        "    # - Normalize the Arabic words\n",
        "    # - Convert the labels to indices\n",
        "    # - Pad the sequences to make them of equal length\n",
        "\n",
        "    # loop over data_list and remove all non arabic words\n",
        "    new_data_list = []\n",
        "    for sentence in data_list:\n",
        "        newSentence = []\n",
        "        for word, label in np.array(sentence).reshape(-1, 2):\n",
        "            if all(ord(char) in range(0x0600, 0x06FF) for char in word):\n",
        "                newSentence.append([word, label])\n",
        "        new_data_list.append(newSentence)\n",
        "\n",
        "    # normalize arabic words (remove elongation)\n",
        "    new_data_list = [\n",
        "        [[normalize_arabic(word), label] for word, label in sent] for sent in new_data_list\n",
        "    ]\n",
        "\n",
        "    # get unique words and labels\n",
        "    labelSet = set()\n",
        "    wordSet = set()\n",
        "    for data in new_data_list:\n",
        "        for word, label in np.array(data).reshape(-1, 2):\n",
        "            labelSet.add(label)\n",
        "            wordSet.add(word.lower())\n",
        "\n",
        "    # get index for each label\n",
        "    labels2index = {t: i for i, t in enumerate(labelSet)}\n",
        "\n",
        "    # X = data | Y = labels\n",
        "    X = [[w[0] for w in s] for s in new_data_list]\n",
        "    Y = [[w[1] for w in s] for s in new_data_list]\n",
        "\n",
        "    # get w2v for the words that are available in the model\n",
        "    X_w2v = []\n",
        "    Y_w2v = [] \n",
        "    for j in range(len(X)):\n",
        "        sentence_x = []\n",
        "        sentence_y = []\n",
        "        for i in range(len(X[j])):\n",
        "            # if word is in the model add it to sentence []\n",
        "            if X[j][i] in model.wv:\n",
        "                sentence_x.append(np.array(model.wv[X[j][i]]))\n",
        "                sentence_y.append(Y[j][i])\n",
        "        # add w2v list (sentence_x) to list of all w2v's (X_train)\n",
        "        Y_w2v.append(sentence_y)\n",
        "        X_w2v.append(sentence_x)\n",
        "\n",
        "    # Get biggest sentence len\n",
        "    #     largest_sen = max(len(sen) for sen in X_w2v)\n",
        "\n",
        "    # 180 is the max sentence length in the training data \n",
        "    # the max in testing data was smaller than 180 so we used the largest.\n",
        "    largest_sen = 180\n",
        "    print(\"biggest sentence has {} words\".format(largest_sen))\n",
        "\n",
        "    # padding (making each sentence the same len as largest_sen)\n",
        "    new_X = []\n",
        "    temp_y = []\n",
        "    for seq_x, seq_y in zip(X_w2v, Y_w2v):\n",
        "        new_seq_x = []\n",
        "        new_seq_y = []\n",
        "        for i in range(largest_sen):\n",
        "            try:\n",
        "                new_seq_x.append(seq_x[i])\n",
        "                new_seq_y.append(seq_y[i])\n",
        "            except:\n",
        "                # if the sentence is smaller than the largest sentence\n",
        "                # we will add words with zeros vector to the small sentence and make its label =(\"0\")\n",
        "                new_seq_x.append(np.zeros((300)))\n",
        "                new_seq_y.append(\"O\")\n",
        "\n",
        "        new_X.append(new_seq_x)\n",
        "        temp_y.append(new_seq_y)\n",
        "        \n",
        "    print(np.array(new_X).shape,np.array(temp_y).shape)\n",
        "    \n",
        "    trunc_X= []\n",
        "    trunc_y=[]\n",
        "\n",
        "    for x, y in zip(new_X, temp_y):\n",
        "        count = 0\n",
        "        for i in range(largest_sen):\n",
        "            if y[i] == \"O\":\n",
        "                count+=1\n",
        "        if count != largest_sen:\n",
        "            trunc_X.append(x)\n",
        "            trunc_y.append(y)\n",
        "    \n",
        "    print(np.array(trunc_X).shape,np.array(trunc_y).shape)\n",
        "\n",
        "    # convert labels to indices\n",
        "    new_y = []\n",
        "    for s in trunc_y:\n",
        "        temp = []\n",
        "        for i, _ in enumerate(s):\n",
        "            temp.append(labels2index[s[i]])\n",
        "        new_y.append(temp)\n",
        "\n",
        "    return np.array(trunc_X), np.array(new_y), labels2index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqavM9dV_Ue9"
      },
      "source": [
        "# **Loading w2v model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3TFGZd5I_XFj"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "model_w2v = gensim.models.Word2Vec.load('./full_uni_cbow_300_twitter.mdl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhLqGcOy_dMI"
      },
      "source": [
        "# **Apply preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1XpJvd8s_bUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb15415-b541-4948-f830-1c55b06b7d76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biggest sentence has 180 words\n",
            "(1976, 180, 300) (1976, 180)\n",
            "(1192, 180, 300) (1192, 180)\n",
            "biggest sentence has 180 words\n",
            "(711, 180, 300) (711, 180)\n",
            "(494, 180, 300) (494, 180)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train,y_train,labels2index=preprocessing(split_train,model_w2v)\n",
        "X_test,y_test,labels2index=preprocessing(split_test,model_w2v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0PiCVZB_mJr"
      },
      "source": [
        "# **One-Hot encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wBcjIofr_o7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02836ace-5ac5-42bc-8d69-ed65d89b1ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape:  (1192, 180, 300) y_train shape:  (1192, 180, 28)\n"
          ]
        }
      ],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train, 28)\n",
        "y_test = to_categorical(y_test, 28)\n",
        "\n",
        "print(\"x_train shape: \",X_train.shape,\"y_train shape: \",y_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GI46B9QZGFet"
      },
      "outputs": [],
      "source": [
        "vocabs = model_w2v.wv.index_to_key\n",
        "vocab_size=len(vocabs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl2ng9AX_x77"
      },
      "source": [
        "# **Building the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9Eh4yK1h_xOy"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import SpatialDropout1D\n",
        "from keras.layers import Dropout\n",
        "from keras import Model, Input\n",
        "from keras.layers import TimeDistributed, Bidirectional\n",
        "from keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KVnMNkSp_6Nv"
      },
      "outputs": [],
      "source": [
        "input_word = Input(shape=(180, 300))\n",
        "lstm1 = Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.2))(input_word)\n",
        "dropout1 = Dropout(0.2)(lstm1)\n",
        "lstm2 = LSTM(units=64, return_sequences=True, recurrent_dropout=0.2)(dropout1)\n",
        "dropout2 = Dropout(0.2)(lstm2)\n",
        "dense_layer = TimeDistributed(Dense(units=10))(dropout2)\n",
        "out = TimeDistributed(Dense(28, activation='softmax'))(dense_layer)\n",
        "model = Model(input_word, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bfUToIi-ACNs"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"Accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Sm4jzx7dASdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b507096d-32ad-441c-c360-4f8d8d0311ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1192, 180, 300) (1192, 180, 28) (494, 180, 28)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,y_train.shape,y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_iuUvBHJAFU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d226e8-861f-46a3-e17b-6d244b4dc51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "15/15 [==============================] - 55s 3s/step - loss: 2.1384 - Accuracy: 0.9075 - val_loss: 0.5982 - val_Accuracy: 0.9773\n",
            "Epoch 2/5\n",
            "15/15 [==============================] - 44s 3s/step - loss: 0.3963 - Accuracy: 0.9765 - val_loss: 0.2328 - val_Accuracy: 0.9773\n",
            "Epoch 3/5\n",
            "15/15 [==============================] - 46s 3s/step - loss: 0.2344 - Accuracy: 0.9765 - val_loss: 0.1945 - val_Accuracy: 0.9773\n",
            "Epoch 4/5\n",
            "15/15 [==============================] - 42s 3s/step - loss: 0.2063 - Accuracy: 0.9765 - val_loss: 0.1765 - val_Accuracy: 0.9773\n",
            "Epoch 5/5\n",
            "15/15 [==============================] - 43s 3s/step - loss: 0.1854 - Accuracy: 0.9766 - val_loss: 0.1569 - val_Accuracy: 0.9774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb0af016640>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=5, batch_size=64,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xM3Zz0FDNs8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e9ade4-28d7-44f5-f3f7-ea7d3a305a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 5s 323ms/step - loss: 0.1961 - Accuracy: 0.9732\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.19609348475933075, 0.9732006192207336]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "udrObgp5PhLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9276b262-e86c-47e1-b711-01be420e67c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 5s 265ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uwBNYsI8Q7G0"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_pred, axis= -1).flatten()\n",
        "y_test = np.argmax(y_test, axis= -1).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "emSENMO0Q_h0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19f7da7-79c7-47e8-f0c0-e8f474e8a144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score =  0.9600159142288612\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1=f1_score(y_test, y_pred,average=\"weighted\")\n",
        "print(\"F1 Score = \",f1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"my_model.h5\")"
      ],
      "metadata": {
        "id": "S_cLVybx2MPy"
      },
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}